{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lucia/.virtualenvs/odsc1/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import RMSprop\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "import shap\n",
    "import eli5\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Function to read data from text to dataframe.\n",
    "    :param filename: path and name of the file to read\n",
    "    :type filename: str\n",
    "    :return: dataframe of the file\n",
    "    :rtype: pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    with open(filename, encoding='utf-8-sig') as txt:\n",
    "        data = txt.read()\n",
    "    data = [x.split('\\t') for x in data.split('\\n')]\n",
    "    data = pd.DataFrame(data[1::], columns=data[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_demographics = '10000_patients/PatientCorePopulatedTable.txt'\n",
    "demographics = read_data(filename_demographics)\n",
    "\n",
    "filename_admissions = '10000_patients/AdmissionsCorePopulatedTable.txt'\n",
    "admissions = read_data(filename_admissions)\n",
    "\n",
    "filename_diagnoses = '10000_patients/AdmissionsDiagnosesCorePopulatedTable.txt'\n",
    "diagnoses = read_data(filename_diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data\n",
    "data = diagnoses.merge(admissions, how='left', on=['PatientID', 'AdmissionID']).merge(\n",
    "    demographics, how='left', on='PatientID')\n",
    "\n",
    "null_data = [i for i in data.index if not data.PatientID[i]]\n",
    "data.loc[null_data]\n",
    "\n",
    "# delete null data\n",
    "data = data.drop(null_data, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dt(date):\n",
    "    if not date:\n",
    "        return None\n",
    "    return datetime.strptime(date, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def get_deltadays(date1, date2):\n",
    "    if not date1 or not date2:\n",
    "        return None\n",
    "    return (convert_dt(date1) - convert_dt(date2)).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate age at admission\n",
    "data['admission_age'] = [get_deltadays(data.AdmissionStartDate[i], data.PatientDateOfBirth[i])/365 for i in \n",
    "                        data.index]\n",
    "\n",
    "# calculate the number of admitted days\n",
    "data['admitted_days'] = [get_deltadays(data.AdmissionEndDate[i], data.AdmissionStartDate[i]) for i in \n",
    "                        data.index]\n",
    "\n",
    "# engineer ICD10 main diagnoses\n",
    "data['icd10'] = [x[0:2] for x in data.PrimaryDiagnosisCode]\n",
    "\n",
    "# encode gender\n",
    "data['gender'] = [0 if x == 'Male' else 1 for x in data.PatientGender]\n",
    "\n",
    "# long admissions\n",
    "data['long_admission'] = [0 if x <= 2 else 1 for x in data.admitted_days]\n",
    "\n",
    "data = data.sort_values(by=['PatientID', 'admission_age']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>AdmissionID</th>\n",
       "      <th>PrimaryDiagnosisCode</th>\n",
       "      <th>PrimaryDiagnosisDescription</th>\n",
       "      <th>AdmissionStartDate</th>\n",
       "      <th>AdmissionEndDate</th>\n",
       "      <th>PatientGender</th>\n",
       "      <th>PatientDateOfBirth</th>\n",
       "      <th>PatientRace</th>\n",
       "      <th>PatientMaritalStatus</th>\n",
       "      <th>PatientLanguage</th>\n",
       "      <th>PatientPopulationPercentageBelowPoverty</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>admitted_days</th>\n",
       "      <th>icd10</th>\n",
       "      <th>tumor</th>\n",
       "      <th>gender</th>\n",
       "      <th>long_admission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>1</td>\n",
       "      <td>F51.04</td>\n",
       "      <td>Psychophysiologic insomnia</td>\n",
       "      <td>1981-05-21 05:21:14.380</td>\n",
       "      <td>1981-05-26 11:13:06.313</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>English</td>\n",
       "      <td>14.63</td>\n",
       "      <td>26.520548</td>\n",
       "      <td>5</td>\n",
       "      <td>F5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>2</td>\n",
       "      <td>M05.33</td>\n",
       "      <td>Rheumatoid heart disease with rheumatoid arthr...</td>\n",
       "      <td>1983-03-22 05:04:47.540</td>\n",
       "      <td>1983-03-26 04:24:25.987</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>English</td>\n",
       "      <td>14.63</td>\n",
       "      <td>28.356164</td>\n",
       "      <td>3</td>\n",
       "      <td>M0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>3</td>\n",
       "      <td>B95.3</td>\n",
       "      <td>Streptococcus pneumoniae as the cause of disea...</td>\n",
       "      <td>1997-03-26 20:04:15.043</td>\n",
       "      <td>1997-03-30 13:08:15.633</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>English</td>\n",
       "      <td>14.63</td>\n",
       "      <td>42.380822</td>\n",
       "      <td>3</td>\n",
       "      <td>B9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>4</td>\n",
       "      <td>F68.12</td>\n",
       "      <td>Factitious disorder with predominantly physica...</td>\n",
       "      <td>2003-09-09 04:17:31.027</td>\n",
       "      <td>2003-09-27 08:13:34.593</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>English</td>\n",
       "      <td>14.63</td>\n",
       "      <td>48.838356</td>\n",
       "      <td>18</td>\n",
       "      <td>F6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>5</td>\n",
       "      <td>F64</td>\n",
       "      <td>Gender identity disorders</td>\n",
       "      <td>2004-03-27 01:01:29.530</td>\n",
       "      <td>2004-04-07 14:52:36.153</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>English</td>\n",
       "      <td>14.63</td>\n",
       "      <td>49.386301</td>\n",
       "      <td>11</td>\n",
       "      <td>F6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              PatientID AdmissionID PrimaryDiagnosisCode  \\\n",
       "0  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           1               F51.04   \n",
       "1  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           2               M05.33   \n",
       "2  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           3                B95.3   \n",
       "3  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           4               F68.12   \n",
       "4  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           5                  F64   \n",
       "\n",
       "                         PrimaryDiagnosisDescription       AdmissionStartDate  \\\n",
       "0                         Psychophysiologic insomnia  1981-05-21 05:21:14.380   \n",
       "1  Rheumatoid heart disease with rheumatoid arthr...  1983-03-22 05:04:47.540   \n",
       "2  Streptococcus pneumoniae as the cause of disea...  1997-03-26 20:04:15.043   \n",
       "3  Factitious disorder with predominantly physica...  2003-09-09 04:17:31.027   \n",
       "4                          Gender identity disorders  2004-03-27 01:01:29.530   \n",
       "\n",
       "          AdmissionEndDate PatientGender       PatientDateOfBirth PatientRace  \\\n",
       "0  1981-05-26 11:13:06.313          Male  1954-11-18 14:15:38.637       White   \n",
       "1  1983-03-26 04:24:25.987          Male  1954-11-18 14:15:38.637       White   \n",
       "2  1997-03-30 13:08:15.633          Male  1954-11-18 14:15:38.637       White   \n",
       "3  2003-09-27 08:13:34.593          Male  1954-11-18 14:15:38.637       White   \n",
       "4  2004-04-07 14:52:36.153          Male  1954-11-18 14:15:38.637       White   \n",
       "\n",
       "  PatientMaritalStatus PatientLanguage  \\\n",
       "0              Married         English   \n",
       "1              Married         English   \n",
       "2              Married         English   \n",
       "3              Married         English   \n",
       "4              Married         English   \n",
       "\n",
       "  PatientPopulationPercentageBelowPoverty  admission_age  admitted_days icd10  \\\n",
       "0                                   14.63      26.520548              5    F5   \n",
       "1                                   14.63      28.356164              3    M0   \n",
       "2                                   14.63      42.380822              3    B9   \n",
       "3                                   14.63      48.838356             18    F6   \n",
       "4                                   14.63      49.386301             11    F6   \n",
       "\n",
       "   tumor  gender  long_admission  \n",
       "0      0       0               1  \n",
       "1      0       0               1  \n",
       "2      0       0               1  \n",
       "3      0       0               1  \n",
       "4      0       0               1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>AdmissionID</th>\n",
       "      <th>PrimaryDiagnosisCode</th>\n",
       "      <th>PrimaryDiagnosisDescription</th>\n",
       "      <th>AdmissionStartDate</th>\n",
       "      <th>AdmissionEndDate</th>\n",
       "      <th>PatientGender</th>\n",
       "      <th>PatientDateOfBirth</th>\n",
       "      <th>PatientRace</th>\n",
       "      <th>PatientMaritalStatus</th>\n",
       "      <th>...</th>\n",
       "      <th>admitted_days</th>\n",
       "      <th>icd10</th>\n",
       "      <th>tumor</th>\n",
       "      <th>gender</th>\n",
       "      <th>long_admission</th>\n",
       "      <th>cat_PatientRace</th>\n",
       "      <th>cat_PatientMaritalStatus</th>\n",
       "      <th>cat_PatientLanguage</th>\n",
       "      <th>cat_PatientPopulationPercentageBelowPoverty</th>\n",
       "      <th>cat_icd10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>1</td>\n",
       "      <td>F51.04</td>\n",
       "      <td>Psychophysiologic insomnia</td>\n",
       "      <td>1981-05-21 05:21:14.380</td>\n",
       "      <td>1981-05-26 11:13:06.313</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>F5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>588</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>2</td>\n",
       "      <td>M05.33</td>\n",
       "      <td>Rheumatoid heart disease with rheumatoid arthr...</td>\n",
       "      <td>1983-03-22 05:04:47.540</td>\n",
       "      <td>1983-03-26 04:24:25.987</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>M0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>588</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>3</td>\n",
       "      <td>B95.3</td>\n",
       "      <td>Streptococcus pneumoniae as the cause of disea...</td>\n",
       "      <td>1997-03-26 20:04:15.043</td>\n",
       "      <td>1997-03-30 13:08:15.633</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>B9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>588</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>4</td>\n",
       "      <td>F68.12</td>\n",
       "      <td>Factitious disorder with predominantly physica...</td>\n",
       "      <td>2003-09-09 04:17:31.027</td>\n",
       "      <td>2003-09-27 08:13:34.593</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>F6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>588</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002C021-6DDD-433A-B9A0-2D26DF0C6AC0</td>\n",
       "      <td>5</td>\n",
       "      <td>F64</td>\n",
       "      <td>Gender identity disorders</td>\n",
       "      <td>2004-03-27 01:01:29.530</td>\n",
       "      <td>2004-04-07 14:52:36.153</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954-11-18 14:15:38.637</td>\n",
       "      <td>White</td>\n",
       "      <td>Married</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>F6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>588</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              PatientID AdmissionID PrimaryDiagnosisCode  \\\n",
       "0  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           1               F51.04   \n",
       "1  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           2               M05.33   \n",
       "2  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           3                B95.3   \n",
       "3  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           4               F68.12   \n",
       "4  0002C021-6DDD-433A-B9A0-2D26DF0C6AC0           5                  F64   \n",
       "\n",
       "                         PrimaryDiagnosisDescription       AdmissionStartDate  \\\n",
       "0                         Psychophysiologic insomnia  1981-05-21 05:21:14.380   \n",
       "1  Rheumatoid heart disease with rheumatoid arthr...  1983-03-22 05:04:47.540   \n",
       "2  Streptococcus pneumoniae as the cause of disea...  1997-03-26 20:04:15.043   \n",
       "3  Factitious disorder with predominantly physica...  2003-09-09 04:17:31.027   \n",
       "4                          Gender identity disorders  2004-03-27 01:01:29.530   \n",
       "\n",
       "          AdmissionEndDate PatientGender       PatientDateOfBirth PatientRace  \\\n",
       "0  1981-05-26 11:13:06.313          Male  1954-11-18 14:15:38.637       White   \n",
       "1  1983-03-26 04:24:25.987          Male  1954-11-18 14:15:38.637       White   \n",
       "2  1997-03-30 13:08:15.633          Male  1954-11-18 14:15:38.637       White   \n",
       "3  2003-09-27 08:13:34.593          Male  1954-11-18 14:15:38.637       White   \n",
       "4  2004-04-07 14:52:36.153          Male  1954-11-18 14:15:38.637       White   \n",
       "\n",
       "  PatientMaritalStatus  ... admitted_days icd10  tumor  gender long_admission  \\\n",
       "0              Married  ...             5    F5      0       0              1   \n",
       "1              Married  ...             3    M0      0       0              1   \n",
       "2              Married  ...             3    B9      0       0              1   \n",
       "3              Married  ...            18    F6      0       0              1   \n",
       "4              Married  ...            11    F6      0       0              1   \n",
       "\n",
       "   cat_PatientRace  cat_PatientMaritalStatus  cat_PatientLanguage  \\\n",
       "0                3                         1                    0   \n",
       "1                3                         1                    0   \n",
       "2                3                         1                    0   \n",
       "3                3                         1                    0   \n",
       "4                3                         1                    0   \n",
       "\n",
       "   cat_PatientPopulationPercentageBelowPoverty  cat_icd10  \n",
       "0                                          588         45  \n",
       "1                                          588        103  \n",
       "2                                          588         15  \n",
       "3                                          588         46  \n",
       "4                                          588         46  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interesting_columns = ['gender', 'PatientRace', 'PatientMaritalStatus', 'PatientLanguage', \n",
    "                       'PatientPopulationPercentageBelowPoverty', 'admission_age', 'admitted_days', 'icd10']\n",
    "object_columns = [x for x in data.select_dtypes(['O']).columns if x in interesting_columns]\n",
    "for col in object_columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "cat_cols = data.select_dtypes(['category']).columns\n",
    "data['cat_' + cat_cols] = data[cat_cols].apply(lambda x: x.cat.codes)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the features we want to consider as columns:\n",
    "- 'admission_age',\n",
    "- 'admitted_days', \n",
    "- 'icd10', \n",
    "- 'gender', \n",
    "- 'cat_PatientRace',\n",
    "- 'cat_PatientMaritalStatus', \n",
    "- 'cat_PatientLanguage',\n",
    "- 'PatientPopulationPercentageBelowPoverty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects, last_age, tot_admissions, long_admissions = [], [], [], []\n",
    "gender, race, status, language, poverty, past_diagnosis = [], [], [], [], [], []\n",
    "no_admissions = []\n",
    "\n",
    "for subj in set(data.PatientID):\n",
    "    temp = data.loc[data.PatientID == subj]\n",
    "    \n",
    "    temp_index = temp.index.tolist()\n",
    "    \n",
    "    if len(temp_index) > 1:\n",
    "\n",
    "        s_last_age = temp.admission_age[temp_index[-2]]\n",
    "        s_tot_admissions = len(temp_index) - 1\n",
    "        s_long_admissions = sum(temp.long_admission.tolist()[0:-1]) / s_tot_admissions\n",
    "        s_gender = temp.gender[temp_index[-1]]\n",
    "        s_race = temp.cat_PatientRace[temp_index[-1]]\n",
    "        s_status = temp.cat_PatientMaritalStatus[temp_index[-1]]\n",
    "        s_language = temp.cat_PatientLanguage[temp_index[-1]]\n",
    "        s_poverty = float(temp.PatientPopulationPercentageBelowPoverty[temp_index[-1]])\n",
    "        s_past_diagnosis = ' '.join(temp.icd10.tolist()[0:-1])\n",
    "\n",
    "        s_no_admissions = 1 if temp.admission_age[temp_index[-1]] - temp.admission_age[temp_index[-2]] < 7 else 0\n",
    "\n",
    "        subjects.append(subj)\n",
    "\n",
    "        # features\n",
    "        last_age.append(s_last_age)\n",
    "        tot_admissions.append(s_tot_admissions)\n",
    "        long_admissions.append(s_long_admissions)\n",
    "        gender.append(s_gender)\n",
    "        race.append(s_race)\n",
    "        status.append(s_status)\n",
    "        language.append(s_language)\n",
    "        poverty.append(s_poverty)\n",
    "        past_diagnosis.append(s_past_diagnosis)\n",
    "\n",
    "        # label\n",
    "\n",
    "        no_admissions.append(s_no_admissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict: is the patient going to be healthy for the next x years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_healthy = pd.DataFrame({'last_age': last_age, 'tot_admissions': tot_admissions, \n",
    "                         'long_admissions': long_admissions, 'gender': gender, 'race': race, 'status': status,\n",
    "                         'language': language, 'poverty': poverty, 'past_diagnosis': past_diagnosis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_healthy, no_admissions, test_size=0.33, \n",
    "                                                    random_state=42, stratify=no_admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase=False)\n",
    "count_vect.fit(X_train.past_diagnosis)\n",
    "\n",
    "col_names = [f'ICD10_{x}' for x in count_vect.vocabulary_]\n",
    "\n",
    "past_diagnosis_train = count_vect.transform(X_train.past_diagnosis)\n",
    "past_diagnosis_train = pd.DataFrame(past_diagnosis_train.todense(), columns = col_names, index=X_train.index)\n",
    "\n",
    "past_diagnosis_valid = count_vect.transform(X_test.past_diagnosis)\n",
    "past_diagnosis_valid = pd.DataFrame(past_diagnosis_valid.todense(), columns = col_names, index=X_test.index)\n",
    "\n",
    "X_train1 = pd.concat([X_train, past_diagnosis_train], axis=1).drop(['past_diagnosis'], axis=1)\n",
    "X_test1 = pd.concat([X_test, past_diagnosis_valid], axis=1).drop(['past_diagnosis'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9768262737875998, 1: 1.024299967814612}\n"
     ]
    }
   ],
   "source": [
    "# balance the classes\n",
    "class_weight = compute_class_weight(\"balanced\", np.unique(y_train), y_train)\n",
    "class_weight_dict = dict(enumerate(class_weight))\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(model, X, y, Xtest, ytest):\n",
    "    model = model.fit(X, y)\n",
    "    test_preds = model.predict(Xtest)\n",
    "    test_scores = model.predict_proba(Xtest)\n",
    "    \n",
    "    print('Confusion matrix for the model')\n",
    "    print(confusion_matrix(y_test, test_preds))\n",
    "    prec, rec, thresholds = precision_recall_curve(ytest, test_scores[:, 1])\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(f\"Precision-Recall AUC: {pr_auc}\")\n",
    "    plt.plot(rec, prec)\n",
    "    plt.title(f\"Precision-Recall AUC: {pr_auc}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(class_weight=class_weight_dict, max_iter=10000)\n",
    "model_lr = fit_and_predict(model_lr, X_train1, y_train, X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting interpretation of a white box model\n",
    "lr_coef = model_lr.coef_[0]\n",
    "sorted_idx = lr_coef.argsort()\n",
    "# get only values that show significance\n",
    "sorted_idx = [i for i in sorted_idx if lr_coef[i] > 0.5 or lr_coef[i] < -0.5]\n",
    "\n",
    "sorted_features = [X_train1.columns[i] for i in sorted_idx]\n",
    "sorted_importance = [model_lr.coef_[0][i] for i in sorted_idx]\n",
    "\n",
    "y_ticks = np.arange(0, len(sorted_features))\n",
    "fig, ax = plt.subplots(figsize=(7, 10))\n",
    "ax.barh(y_ticks, sorted_importance)\n",
    "ax.set_yticklabels(sorted_features)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Feature Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple black box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(class_weight=class_weight_dict)\n",
    "model_rf = fit_and_predict(model_rf, X_train1, y_train, X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree interpreter\n",
    "instances = X_test1.loc[[x for x in X_test1.index if x in [4350, 9408]]]\n",
    "print(f\"Instances prediction: {model_rf.predict(instances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, bias, contributions = ti.predict(model_rf, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(instances)):\n",
    "    print(\"Instance\", i)\n",
    "    print(\"Bias (trainset mean)\", bias[i])\n",
    "    print(\"Feature contributions:\")\n",
    "    for c, feature in sorted(zip(contributions[i][:, 1], \n",
    "                                 X_test1.columns), \n",
    "                             key=lambda x: -abs(x[0])):\n",
    "        if abs(c) > 0.01:\n",
    "            print(feature, round(c, 2))\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "sorted_idx = model_rf.feature_importances_.argsort()\n",
    "# get only values that show significance\n",
    "sorted_idx = [i for i in sorted_idx if model_rf.feature_importances_[i] > 0.005]\n",
    "\n",
    "sorted_features = [X_train1.columns[i] for i in sorted_idx]\n",
    "sorted_importance = [model_rf.feature_importances_[i] for i in sorted_idx]\n",
    "\n",
    "y_ticks = np.arange(0, len(sorted_features))\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "ax.barh(y_ticks, sorted_importance)\n",
    "ax.set_yticklabels(sorted_features)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Feature Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(model_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Actual Label:', y_test[0])\n",
    "print('Predicted Label:', model_rf.predict_proba(X_test1.loc[X_test1.index[0:1]])[0][1])\n",
    "eli5.show_prediction(model_rf, X_test1.loc[X_test1.index[0:1]], \n",
    "                     feature_names=list(X_test1.columns),\n",
    "                     show_feature_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = Interpretation(training_data=X_test1, training_labels=y_test, \n",
    "                             feature_names=list(X_test1.columns))\n",
    "im_model = InMemoryModel(model_rf.predict_proba, examples=X_train1, \n",
    "                         # target_names=['$50K or less', 'More than $50K']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = interpreter.feature_importance.plot_feature_importance(im_model, ascending=True, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial dependency plot\n",
    "\n",
    "r = interpreter.partial_dependence.plot_partial_dependence(['tot_admissions'], im_model, grid_resolution=50, \n",
    "                                                           grid_range=(0,1),  \n",
    "                                                           with_variance=True, figsize = (6, 4))\n",
    "yl = r[0][1].set_ylim(0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = interpreter.partial_dependence.plot_partial_dependence(['last_age'], im_model, grid_resolution=50, \n",
    "                                                           grid_range=(0,1),  \n",
    "                                                           with_variance=True, figsize = (6, 4))\n",
    "yl = r[0][1].set_ylim(0, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_dict_xgb = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "\n",
    "model_xgb = XGBClassifier(objective='binary:logistic', scale_pos_weight=class_weight_dict_xgb)\n",
    "model_xgb = fit_and_predict(model_xgb, X_train1, y_train, X_test1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-house feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 30))\n",
    "title = fig.suptitle(\"Default Feature Importances from XGBoost\", fontsize=14)\n",
    "\n",
    "ax1 = fig.add_subplot(2,2, 1)\n",
    "xgboost.plot_importance(model_xgb, importance_type='weight', ax=ax1)\n",
    "t=ax1.set_title(\"Feature Importance - Feature Weight\")\n",
    "\n",
    "ax2 = fig.add_subplot(2,2, 2)\n",
    "xgboost.plot_importance(model_xgb, importance_type='gain', ax=ax2)\n",
    "t=ax2.set_title(\"Feature Importance - Split Mean Gain\")\n",
    "\n",
    "ax3 = fig.add_subplot(2,2, 3)\n",
    "xgboost.plot_importance(model_xgb, importance_type='cover', ax=ax3)\n",
    "t=ax3.set_title(\"Feature Importance - Sample Coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skater feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = Interpretation(training_data=X_test1, training_labels=y_test, \n",
    "                             feature_names=list(X_test1.columns))\n",
    "im_model = InMemoryModel(model_xgb.predict_proba, examples=X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = interpreter.feature_importance.plot_feature_importance(im_model, ascending=True) # 400 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = interpreter.partial_dependence.plot_partial_dependence(['poverty'], im_model, grid_resolution=50, \n",
    "                                                           grid_range=(0,1), with_variance=True, figsize = (6, 4))\n",
    "yl = r[0][1].set_ylim(0, 1) # 42 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_list = interpreter.partial_dependence.plot_partial_dependence([('tot_admissions', 'status')], \n",
    "                                                                    im_model, grid_range=(0,1), \n",
    "                                                                    figsize=(12, 5),\n",
    "                                                                    grid_resolution=100) # 110 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find good predicted\n",
    "predictions = model_xgb.predict(X_test1)\n",
    "pred0_index = [i for i,x in enumerate(predictions) if x == y_test[i] and y_test[i] == 0]\n",
    "pred1_index = [i for i,x in enumerate(predictions) if x == y_test[i] and y_test[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME with Skater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Skater can leverage LIME to explain model predictions. \n",
    "Typically, its LimeTabularExplainer class helps in explaining predictions on tabular (i.e. matrix) data. \n",
    "For numerical features, it perturbs them by sampling from a Normal(0,1) \n",
    "and doing the inverse operation of mean-centering and scaling, \n",
    "according to the means and stds in the training data. For categorical features, \n",
    "it perturbs by sampling according to the training distribution, \n",
    "and making a binary feature that is 1 when the value is the same as the instance being explained. \n",
    "The explain_instance() function generates explanations for a prediction. \n",
    "First, we generate neighborhood data by randomly perturbing features from the instance. \n",
    "We then learn locally weighted linear (surrogate) \n",
    "models on this neighborhood data to explain each of the classes in an interpretable way.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = LimeTabularExplainer(X_test1.values, feature_names=list(X_test1.columns), \n",
    "                           discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_value = XGBClassifier(objective='binary:logistic', scale_pos_weight=class_weight_dict_xgb)\n",
    "model_xgb_value.fit(X_train1.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual Label:', y_test[pred0_index[0]])\n",
    "print('Predicted Label:', predictions[pred0_index[0]])\n",
    "exp.explain_instance(X_test1.loc[pred0_index[0]].values, \n",
    "                     model_xgb_value.predict_proba).show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual Label:', y_test[pred1_index[0]])\n",
    "print('Predicted Label:', predictions[pred1_index[0]])\n",
    "exp.explain_instance(X_test1.loc[pred1_index[0]].values, \n",
    "                     model_xgb_value.predict_proba).show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREPAN in skater\n",
    "# The implementation also generates a fidelity score to quantify tree based surrogate model’s \n",
    "# approximation to the Oracle. Ideally, the score should be 0 for truthful explanation \n",
    "# both globally and locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_explainer = interpreter.tree_surrogate(oracle=im_model, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_explainer.fit(X_train1, y_train, use_oracle=True, prune='pre', scorer_type='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([('X'+str(idx), feature) \n",
    "                   for (idx, feature) in enumerate(X_train1.columns)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skater.util.dataops import show_in_notebook\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Source(surrogate_explainer.plot_global_decisions(colors=['coral', 'darkturquoise'], \n",
    "                                          file_name='test_tree_pre.png').to_string())\n",
    "svg_data = graph.pipe(format='svg')\n",
    "with open('dtree_structure.svg','wb') as f:\n",
    "    f.write(svg_data)\n",
    "SVG(svg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_xgb)\n",
    "shap_values = explainer.shap_values(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap for base prediction\n",
    "# The following is the expected probability of something being \n",
    "# classified as Class 1. explainer.expected_value[0] gives the \n",
    "# expected probability of something being classfied as Class 0. Any # SHAP value contributes towards or against this base expected \n",
    "# probability, which is calcultated for the dataset, not for the \n",
    "# model.\n",
    "print('Expected Value:', explainer.expected_value)\n",
    "pd.DataFrame(shap_values).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_test1, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[pred0_index[0],:], X_test1.iloc[pred0_index[0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[pred1_index[0],:], X_test1.iloc[pred1_index[0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, \n",
    "                shap_values[:1000,:], X_test1.loc[:1000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test1, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(ind='tot_admissions', interaction_index='tot_admissions',\n",
    "                     shap_values=shap_values, \n",
    "                     features=X_test1,  \n",
    "                     display_features=X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(ind='last_age', interaction_index='last_age',\n",
    "                     shap_values=shap_values, \n",
    "                     features=X_test1,  \n",
    "                     display_features=X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(ind='tot_admissions', interaction_index='last_age', \n",
    "                     shap_values=shap_values, features=X_test1, \n",
    "                     display_features=X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dense_model(initial_length):\n",
    "    \"\"\"\n",
    "    Function to create a multilayer deep fully connected neural network\n",
    "    :param initial_length: length of the input vectors\n",
    "    :type initial_length: int\n",
    "    :return: model according to input size\n",
    "    :rtype: tensorflow.python.keras.engine.training.Model\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(initial_length,), name=\"input\")\n",
    "    # normalized the batches\n",
    "    x = BatchNormalization(name='input_bn')(inputs)\n",
    "    x = Dense(256, activation=\"softmax\", name=\"dense1\")(x)\n",
    "    x = Dropout(0.5, name=\"dropout1\")(x)\n",
    "    x = Dense(128, activation=\"softmax\", name=\"dense2\")(x)\n",
    "    x = Dropout(0.5, name=\"dropout2\")(x)\n",
    "    x = Dense(256, activation=\"softmax\", name=\"dense3\")(inputs)\n",
    "    x = Dropout(0.2, name=\"dropout3\")(x)\n",
    "    output = Dense(2, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    # Compile model and create model summary\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=RMSprop(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "    # model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lucia/.virtualenvs/odsc1/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "dense = build_dense_model(X_train1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lucia/.virtualenvs/odsc1/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/lucia/.virtualenvs/odsc1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 5092 samples, validate on 1273 samples\n",
      "Epoch 1/3\n",
      "5092/5092 [==============================] - 1s 171us/step - loss: 0.6910 - accuracy: 0.4853 - val_loss: 0.6890 - val_accuracy: 0.5027\n",
      "Epoch 2/3\n",
      "5092/5092 [==============================] - 0s 92us/step - loss: 0.6886 - accuracy: 0.4943 - val_loss: 0.6862 - val_accuracy: 0.5153\n",
      "Epoch 3/3\n",
      "5092/5092 [==============================] - 1s 107us/step - loss: 0.6859 - accuracy: 0.5487 - val_loss: 0.6833 - val_accuracy: 0.5962\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=False)\n",
    "checkpoint = ModelCheckpoint('model_dense.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='auto')  \n",
    "\n",
    "history = dense.fit(\n",
    "    x=X_train1, \n",
    "    y=to_categorical(y_train),\n",
    "    validation_split=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=3, #1000,\n",
    "    verbose=1,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = build_dense_model(X_train1.shape[1])\n",
    "dense1.load_weights('model_dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 433 1171]\n",
      " [ 122 1409]]\n",
      "0.6463664930798853\n"
     ]
    }
   ],
   "source": [
    "test_scores = dense1.predict(X_test1)\n",
    "test_preds = [0 if x < 0.5 else 1 for x in test_scores[:, 1]]\n",
    "\n",
    "print(confusion_matrix(y_test, test_preds))\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, test_scores[:, 1])\n",
    "pr_auc = auc(recalls, precisions)\n",
    "print(pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientExplainer**\n",
    "Explains a model using expected gradients (an extension of integrated gradients).\n",
    "\n",
    "Expected gradients an extension of the integrated gradients method (Sundararajan et al. 2017), a feature attribution method designed for differentiable models based on an extension of Shapley values to infinite player games (Aumann-Shapley values). Integrated gradients values are a bit different from SHAP values, and require a single reference value to integrate from. As an adaptation to make them approximate SHAP values, expected gradients reformulates the integral as an expectation and combines that expectation with sampling reference values from the background dataset. This leads to a single combined expectation of gradients that converges to attributions that sum to the difference between the expected model output and the current output.\n",
    "\n",
    "**DeepExplainer**\n",
    "Meant to approximate SHAP values for deep learning models.\n",
    "\n",
    "This is an enhanced version of the DeepLIFT algorithm (Deep SHAP) where, similar to Kernel SHAP, we approximate the conditional expectations of SHAP values using a selection of background samples. Lundberg and Lee, NIPS 2017 showed that the per node attribution rules in DeepLIFT (Shrikumar, Greenside, and Kundaje, arXiv 2017) can be chosen to approximate Shapley values. By integrating over many backgound samples DeepExplainer estimates approximate SHAP values such that they sum up to the difference between the expected model output on the passed background samples and the current model output (f(x) - E[f(x)]).\n",
    "\n",
    "from https://shap.readthedocs.io/en/latest/\n",
    "\n",
    "Note: For tf.keras users, SHAP still does not support tf 2.0.\n",
    "see https://github.com/slundberg/shap/issues/885\n",
    "\n",
    "Use shap.GradientExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/odsc1/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-aaea6151b165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/odsc1/lib/python3.6/site-packages/shap/explainers/deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/odsc1/lib/python3.6/site-packages/shap/explainers/deep/deep_tf.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                     \u001b[0mphis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_phis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0moutput_phis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_input\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mphis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/odsc1/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/odsc1/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "explainer = shap.DeepExplainer(dense1, X_train1)\n",
    "shap_values = explainer.shap_values(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find good predicted\n",
    "pred0_index = [i for i,x in enumerate(test_preds) if x == y_test[i] and y_test[i] == 0]\n",
    "pred1_index = [i for i,x in enumerate(test_preds) if x == y_test[i] and y_test[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.explain_instance(X_test1.loc[pred0_index[0]].values, \n",
    "                     model_xgb_value.predict_proba).show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain predictions of the model on four images\n",
    "e = shap.DeepExplainer(model, background)\n",
    "# ...or pass tensors directly\n",
    "# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\n",
    "shap_values = e.shap_values(x_test[1:5])\n",
    "\n",
    "# plot the feature attributions\n",
    "shap.image_plot(shap_values, -x_test[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
